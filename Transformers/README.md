# DL_Impls
Codes implementation for DL. Self-written or customized reproduction based on offical codes.
All codes base on Pytorch.

## Transformer
[Attention Is All You Need](https://arxiv.org/abs/1706.03762)
* Encoder-Decoder architecture. A era.

## BERT
[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
* Mask modeling. Only encoder.

## ViT
[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)
* Vision Transformer. Based on BERT. Scale Transformer to CV for the first time.

## Swin-Transformer
[Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030)

## MAE
[Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377)
* Self-supervised learning for CV. Based on ViT. Scale mian CV tasks to SOTA.
